
#include "Enclave.h"
#include "stdioPatched.h"

#include <stdarg.h>
#include <string.h>

#include "libs/tensorflow/lite/micro/all_ops_resolver.h"
#include "libs/tensorflow/lite/micro/micro_error_reporter.h"
#include "libs/tensorflow/lite/micro/micro_interpreter.h"
#include "libs/tensorflow/lite/schema/schema_generated.h"
#include "model.h"

void load_data(float* data, TfLiteTensor * input){
    for (int i = 0; i < input->bytes; ++i){
            input->data.data = data;
        }
}


void perform_inference(char *model_path, char *input_path){
    
    // Setup logging
    tflite::MicroErrorReporter micro_error_reporter;
    tflite::ErrorReporter* error_reporter = &micro_error_reporter;

    // Read the tflight model from disk:
    long fsize;
	int res = get_filesize(model_path, &fsize);
    if (res < 0){
        printf("failed to read model file\n");
        return;
    }

	unsigned char tfmodel[fsize+1];
	int res1 = load_model(model_path, fsize, tfmodel);
     if (res1 < 0){
        printf("failed to read model file\n");
        return;
    }
    printf("[ENCLAVE] loaded model \n");

    // load input data

    // Get input length

    long isize;
    int res3 = get_filesize(input_path, &isize);
    if (res3 < 0){
        printf("failed to read input file\n");
        return;
    }

    float tfinput[isize+1];
    load_input(input_path, isize, tfinput);
    printf("[ENCLAVE] loaded data\n");
    const tflite::Model* model = ::tflite::GetModel(g_model);
    if (model->version() != TFLITE_SCHEMA_VERSION) {
    TF_LITE_REPORT_ERROR(error_reporter,
        "Model provided is schema version %d not equal "
        "to supported version %d.\n",
        model->version(), TFLITE_SCHEMA_VERSION);
    }

    // init op resolver
    tflite::AllOpsResolver resolver;

    // Allocate memory
    const int tensor_arena_size = 50 * 1024;
    uint8_t tensor_arena[tensor_arena_size];

    // init interpreter & allocate tensors
    // Build an interpreter to run the model with.
    static tflite::MicroInterpreter static_interpreter(
        model, resolver, tensor_arena, tensor_arena_size, error_reporter);
    tflite::MicroInterpreter* interpreter = &static_interpreter;
    
    interpreter->AllocateTensors();

    // Obtain a pointer to the model's input tensor
    TfLiteTensor* input = interpreter->input(0);
   
    
    //TODO: Check input is really int8
/*
    for (int i = 0; i < input->bytes; ++i){
            input->data.int8 = x_quantized;
    }*/
    printf("input->data: %p\n", input->data);
    input->data.f[0] = 0.;

    TfLiteStatus invoke_status = interpreter->Invoke();
    if (invoke_status != kTfLiteOk) {
        TF_LITE_REPORT_ERROR(error_reporter, "Invoke failed\n");
    }

    // Check model output
    TfLiteTensor* output = interpreter->output(0);
   
    // Get the output quantization parameters
    float output_scale = output->params.scale;
    int output_zero_point = output->params.zero_point;

    // Obtain the output value from the tensor
    for (int i = 0; i < 1; i++){
        printf("output[%d]=%f\n",i,output->data.f[0]);

    }

}
